#!/usr/bin/env python3
"""
AI æ—¥æŠ¥ç®€æŠ¥ç”Ÿæˆå™¨
æ¯å¤©ä» wechat-article-exporter API æ‹‰å–å…³æ³¨å…¬ä¼—å·çš„æœ€æ–°æ–‡ç« ï¼Œ
ç­›é€‰ AI ç›¸å…³ï¼Œåˆå¹¶åŒç±»é¡¹ï¼Œç”Ÿæˆç®€æŠ¥ä¾›äººå·¥ç²¾é€‰å…¥åº“ã€‚

å…¨è‡ªåŠ¨ï¼šç›´æ¥ä» Chrome Cookies æå– auth-keyï¼Œè°ƒç”¨ exporter APIã€‚
å…¬ä¼—å·åˆ—è¡¨ä» exporter çš„ /api/public/v1/followed-accounts åŠ¨æ€è·å–ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
  source ~/venv/automation/bin/activate
  python auto_import.py              # é»˜è®¤æœ€è¿‘ 24h
  python auto_import.py --hours 48   # æœ€è¿‘ 48h
  python auto_import.py --list-accounts          # æŸ¥çœ‹å…³æ³¨åˆ—è¡¨
  python auto_import.py --add-account "æ–°è´¦å·"    # æ·»åŠ å…¬ä¼—å·
  python auto_import.py --remove-account "æ—§è´¦å·" # ç§»é™¤å…¬ä¼—å·
  python auto_import.py --sync-accounts           # ä» Chrome exporter åŒæ­¥åˆ—è¡¨
  python auto_import.py --mark-read 2026-02-20 1 3 5  # æ ‡è®°å·²è¯»
  python auto_import.py --stats                    # é˜…è¯»ç»Ÿè®¡
  python auto_import.py --trends                   # çƒ­ç‚¹è¶‹åŠ¿
  python auto_import.py --search "Agent"           # æœç´¢æ–‡ç« 
  python auto_import.py --build-site              # æ„å»º/æ›´æ–° digest-site é™æ€ç«™
"""

import argparse
import hashlib
import json
import re
import shutil
import sqlite3
import subprocess
import sys
import tempfile
import time
from datetime import datetime, timedelta
from difflib import SequenceMatcher
from pathlib import Path

import requests

# â”€â”€ è·¯å¾„é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = Path(__file__).resolve().parent
KB_ROOT = SCRIPT_DIR.parent
KNOWLEDGE_BASE_DIR = KB_ROOT / "knowledge_base"
INDEX_DIR = KNOWLEDGE_BASE_DIR / "_index"
ARTICLES_INDEX_FILE = INDEX_DIR / "articles_index.json"
CLASSIFICATION_RULES_FILE = INDEX_DIR / "classification_rules.json"
LOG_DIR = KB_ROOT / "logs"
DEVLOG_FILE = LOG_DIR / "devlog.jsonl"
AUTO_IMPORT_LOG = LOG_DIR / "auto_import.log"
DIGEST_DIR = KB_ROOT / "digests"

EXPORTER_BASE = "http://localhost:3000"
CHROME_COOKIES_DB = (
    Path.home()
    / "Library/Application Support/Google/Chrome/Default/Cookies"
)

FOLLOWED_ACCOUNTS_API = f"{EXPORTER_BASE}/api/public/v1/followed-accounts"

# AI ç›¸å…³æ€§é«˜é¢‘è¯
AI_TITLE_KEYWORDS = [
    "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "å¤§æ¨¡å‹", "LLM", "GPT", "Claude",
    "DeepSeek", "OpenAI", "Agent", "ç®—æ³•", "èŠ¯ç‰‡", "è‹±ä¼Ÿè¾¾",
    "åƒé—®", "è±†åŒ…", "Gemini", "Sora", "ChatGPT", "Copilot",
    "AGI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "Transformer",
    "å¼ºåŒ–å­¦ä¹ ", "è‡ªåŠ¨é©¾é©¶", "å…·èº«æ™ºèƒ½", "å¤šæ¨¡æ€", "Anthropic",
    "Llama", "å¼€æºæ¨¡å‹", "AIGC", "ç”Ÿæˆå¼",
]

SIMILARITY_THRESHOLD = 0.45
API_DELAY = 1.5  # ç§’ï¼ŒAPI è¯·æ±‚é—´éš”
SITE_DIR = KB_ROOT / "digest-site"

READING_PROFILE_FILE = SCRIPT_DIR / "reading_profile.json"
READING_LOG_FILE = LOG_DIR / "reading_log.jsonl"


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def log(msg: str):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(AUTO_IMPORT_LOG, "a", encoding="utf-8") as f:
        f.write(line + "\n")


def notify(title: str, msg: str):
    """macOS åŸç”Ÿé€šçŸ¥"""
    subprocess.run(
        ["osascript", "-e", f'display notification "{msg}" with title "{title}"'],
        capture_output=True,
    )


def _api_get(url: str, params: dict = None, headers: dict = None, retries: int = 3, timeout: int = 10) -> requests.Response:
    """å¸¦æŒ‡æ•°é€€é¿é‡è¯•çš„ API GET è¯·æ±‚ï¼Œä»…å¯¹è¶…æ—¶/è¿æ¥é”™è¯¯é‡è¯•"""
    for attempt in range(retries):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
            if attempt < retries - 1:
                wait = 2 ** (attempt + 1)  # 2s â†’ 4s â†’ 8s
                log(f"  API è¯·æ±‚å¤±è´¥ (attempt {attempt + 1}/{retries}), {wait}s åé‡è¯•: {e}")
                time.sleep(wait)
            else:
                raise
        except requests.exceptions.HTTPError:
            raise  # 4xx/5xx ä¸é‡è¯•


def devlog(entry: dict):
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    entry["timestamp"] = datetime.now().isoformat()
    entry["project"] = "knowledgebase"
    with open(DEVLOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def load_classification_rules() -> dict:
    with open(CLASSIFICATION_RULES_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def load_articles_index() -> dict:
    with open(ARTICLES_INDEX_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


# â”€â”€ Chrome Cookie è§£å¯† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_chrome_safe_storage_password() -> str:
    """ä» macOS Keychain è·å– Chrome Safe Storage å¯†ç """
    result = subprocess.run(
        ["security", "find-generic-password", "-w", "-s", "Chrome Safe Storage", "-a", "Chrome"],
        capture_output=True, text=True
    )
    if result.returncode != 0:
        raise RuntimeError(f"æ— æ³•è·å– Chrome Safe Storage å¯†ç : {result.stderr}")
    return result.stdout.strip()


def _derive_chrome_key(password: str) -> bytes:
    """PBKDF2 æ´¾ç”Ÿ AES å¯†é’¥"""
    return hashlib.pbkdf2_hmac("sha1", password.encode("utf-8"), b"saltysalt", 1003, dklen=16)


def _decrypt_chrome_cookie(encrypted_value: bytes, key: bytes) -> str:
    """è§£å¯† Chrome v10 æ ¼å¼ cookie"""
    from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
    from cryptography.hazmat.primitives import padding

    if encrypted_value[:3] != b"v10":
        raise ValueError("ä¸æ˜¯ v10 æ ¼å¼ cookie")

    encrypted_data = encrypted_value[3:]
    iv = b" " * 16
    cipher = Cipher(algorithms.AES(key), modes.CBC(iv))
    decryptor = cipher.decryptor()
    decrypted = decryptor.update(encrypted_data) + decryptor.finalize()

    # å»æ‰ PKCS7 padding
    unpadder = padding.PKCS7(128).unpadder()
    unpadded = unpadder.update(decrypted) + unpadder.finalize()

    # Chrome cookie è§£å¯†åæœ‰ 32 å­—èŠ‚äºŒè¿›åˆ¶å‰ç¼€ + 32 å­—èŠ‚ ASCII auth-key
    # auth-key æ˜¯ crypto.randomUUID() å»æ‰è¿å­—ç¬¦çš„ 32 hex chars
    try:
        return unpadded.decode("utf-8")
    except UnicodeDecodeError:
        # å–æœ€å 32 å­—èŠ‚ä½œä¸º auth-key
        auth_bytes = unpadded[-32:]
        try:
            return auth_bytes.decode("ascii")
        except Exception:
            raise ValueError(f"æ— æ³•ä»è§£å¯†æ•°æ®ä¸­æå– auth-key: {unpadded.hex()}")


def get_auth_key() -> str:
    """ä» Chrome Cookies DB æå– exporter çš„ auth-key"""
    if not CHROME_COOKIES_DB.exists():
        raise FileNotFoundError(f"Chrome Cookies DB ä¸å­˜åœ¨: {CHROME_COOKIES_DB}")

    # Chrome é”å®š Cookies DBï¼Œéœ€è¦å¤åˆ¶ä¸€ä»½
    tmp = tempfile.mktemp(suffix=".db")
    shutil.copy2(str(CHROME_COOKIES_DB), tmp)

    try:
        conn = sqlite3.connect(tmp)
        cursor = conn.execute(
            "SELECT encrypted_value FROM cookies WHERE host_key = ? AND name = ?",
            ("localhost", "auth-key"),
        )
        row = cursor.fetchone()
        conn.close()

        if not row:
            raise ValueError("Cookies DB ä¸­æ²¡æœ‰ auth-keyï¼ˆéœ€è¦å…ˆåœ¨æµè§ˆå™¨ä¸­ç™»å½• exporterï¼‰")

        password = _get_chrome_safe_storage_password()
        key = _derive_chrome_key(password)
        return _decrypt_chrome_cookie(row[0], key)
    finally:
        Path(tmp).unlink(missing_ok=True)


# â”€â”€ æ£€æŸ¥ exporter æœåŠ¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def check_exporter() -> bool:
    try:
        r = requests.get(EXPORTER_BASE, timeout=5)
        return r.status_code == 200
    except Exception:
        return False


# â”€â”€ API æ‹‰å–æ–‡ç«  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_account(auth_key: str, keyword: str) -> dict | None:
    """æœç´¢å…¬ä¼—å·ï¼Œè¿”å› {fakeid, nickname}"""
    url = f"{EXPORTER_BASE}/api/web/mp/searchbiz"
    params = {"keyword": keyword}
    headers = {"X-Auth-Key": auth_key}

    try:
        r = _api_get(url, params=params, headers=headers)
        data = r.json()
        items = data.get("base_resp", {}).get("ret") if data.get("base_resp") else None
        if items == 0:  # success
            biz_list = data.get("list", [])
            if biz_list:
                return {"fakeid": biz_list[0]["fakeid"], "nickname": biz_list[0]["nickname"]}
        # Fallback: check direct list field
        biz_list = data.get("list", [])
        if biz_list:
            return {"fakeid": biz_list[0]["fakeid"], "nickname": biz_list[0]["nickname"]}
    except Exception as e:
        log(f"  æœç´¢ {keyword} å¤±è´¥: {e}")
    return None


def fetch_account_articles(auth_key: str, fakeid: str, cutoff_ts: int) -> list[dict]:
    """æ‹‰å–æŸå…¬ä¼—å·è‡ª cutoff_ts ä»¥æ¥çš„æ‰€æœ‰æ–‡ç« """
    url = f"{EXPORTER_BASE}/api/web/mp/appmsgpublish"
    headers = {"X-Auth-Key": auth_key}
    articles = []
    begin = 0
    size = 10

    while True:
        params = {"id": fakeid, "begin": begin, "size": size}
        try:
            r = _api_get(url, params=params, headers=headers, timeout=15)
            data = r.json()
        except Exception as e:
            log(f"    page {begin // size + 1} å¤±è´¥: {e}")
            break

        # publish_page æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦äºŒæ¬¡è§£æ
        publish_page_raw = data.get("publish_page", "")
        if not publish_page_raw:
            break

        try:
            publish_page = json.loads(publish_page_raw) if isinstance(publish_page_raw, str) else publish_page_raw
        except json.JSONDecodeError:
            log(f"    publish_page è§£æå¤±è´¥")
            break

        publish_list = publish_page.get("publish_list", [])
        if not publish_list:
            break

        reached_cutoff = False
        for batch in publish_list:
            # publish_info ä¹Ÿæ˜¯ JSON å­—ç¬¦ä¸²
            pub_info_raw = batch.get("publish_info", "")
            try:
                pub_info = json.loads(pub_info_raw) if isinstance(pub_info_raw, str) else pub_info_raw
            except (json.JSONDecodeError, TypeError):
                continue

            for item in pub_info.get("appmsgex", []):
                create_time = item.get("create_time", 0)
                if create_time < cutoff_ts:
                    reached_cutoff = True
                    break
                articles.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "create_time": create_time,
                    "digest": item.get("digest", ""),
                    "fakeid": fakeid,
                    "nickname": "",  # filled by caller
                    "cover": item.get("cover", ""),
                })
            if reached_cutoff:
                break

        if reached_cutoff or len(publish_list) < size:
            break

        begin += size
        time.sleep(API_DELAY)

    return articles


def get_followed_accounts() -> list[str]:
    """ä» exporter API è·å–å…³æ³¨çš„å…¬ä¼—å·åˆ—è¡¨"""
    try:
        r = requests.get(FOLLOWED_ACCOUNTS_API, timeout=5)
        data = r.json()
        accounts = data.get("accounts", [])
        if accounts:
            return accounts
    except Exception as e:
        log(f"ä» exporter è·å–å…¬ä¼—å·åˆ—è¡¨å¤±è´¥: {e}")
    log("å…¬ä¼—å·åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆé€šè¿‡ POST /api/public/v1/followed-accounts è®¾ç½®")
    return []


def sync_accounts_from_chrome() -> list[str]:
    """é€šè¿‡ AppleScript ä» Chrome IndexedDB æ‹‰å– exporter çš„å…¬ä¼—å·åˆ—è¡¨å¹¶åŒæ­¥åˆ° API"""
    import json as _json
    script = '''
    tell application "Google Chrome"
        repeat with w in windows
            repeat with t in tabs of w
                if URL of t starts with "http://localhost:3000" then
                    set jsResult to execute t javascript "
                        var req = indexedDB.open('exporter.wxdown.online');
                        req.onsuccess = function(e) {
                            var db = e.target.result;
                            var tx = db.transaction(['info'], 'readonly');
                            var store = tx.objectStore('info');
                            var getAll = store.getAll();
                            getAll.onsuccess = function() {
                                var names = getAll.result.map(function(a) { return a.nickname || ''; }).filter(Boolean);
                                document.title = JSON.stringify(names);
                            };
                        };
                        'ok'
                    "
                    delay 1
                    return title of t
                end if
            end repeat
        end repeat
        return "NO_TAB"
    end tell
    '''
    result = subprocess.run(["osascript", "-e", script], capture_output=True, text=True)
    raw = result.stdout.strip()
    if not raw or raw == "NO_TAB":
        print("æœªæ‰¾åˆ° exporter æ ‡ç­¾é¡µï¼Œè¯·ç¡®ä¿ Chrome ä¸­æ‰“å¼€äº† localhost:3000")
        return []

    try:
        accounts = _json.loads(raw)
    except Exception:
        print(f"è§£æå¤±è´¥: {raw[:200]}")
        return []

    # åŒæ­¥åˆ° API
    try:
        r = requests.post(FOLLOWED_ACCOUNTS_API, json={"accounts": accounts}, timeout=5)
        data = r.json()
        synced = data.get("accounts", accounts)
        print(f"å·²ä» Chrome åŒæ­¥ {len(synced)} ä¸ªå…¬ä¼—å·åˆ° API")
        for a in synced:
            print(f"  - {a}")
        return synced
    except Exception as e:
        print(f"åŒæ­¥åˆ° API å¤±è´¥: {e}")
        return accounts


def fetch_all_articles(auth_key: str, hours: int) -> list[dict]:
    """æ‹‰å–æ‰€æœ‰å…³æ³¨å…¬ä¼—å·çš„æœ€æ–°æ–‡ç« """
    cutoff_ts = int((datetime.now() - timedelta(hours=hours)).timestamp())
    all_articles = []

    followed = get_followed_accounts()
    if not followed:
        return []

    log(f"å…³æ³¨åˆ—è¡¨ ({len(followed)}): {', '.join(followed)}")

    for name in followed:
        log(f"æœç´¢: {name}")
        account = search_account(auth_key, name)
        time.sleep(API_DELAY)

        if not account:
            log(f"  æœªæ‰¾åˆ°: {name}")
            continue

        log(f"  æ‰¾åˆ°: {account['nickname']} ({account['fakeid'][:8]}...)")
        articles = fetch_account_articles(auth_key, account["fakeid"], cutoff_ts)

        # Fill in nickname
        for a in articles:
            a["nickname"] = account["nickname"]

        log(f"  {account['nickname']}: {len(articles)} ç¯‡")
        all_articles.extend(articles)
        time.sleep(API_DELAY)

    return all_articles


# â”€â”€ AI ç›¸å…³æ€§åˆ¤æ–­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_ai_related(title: str, digest: str) -> bool:
    text = (title + " " + digest).lower()
    title_lower = title.lower()

    for kw in AI_TITLE_KEYWORDS:
        if kw.lower() in title_lower:
            return True

    rules = load_classification_rules()
    ai_keywords = rules["categories"].get("äººå·¥æ™ºèƒ½", {}).get("keywords", [])
    match_count = sum(1 for kw in ai_keywords if kw.lower() in text)
    return match_count >= 2


# â”€â”€ åˆå¹¶åŒç±»é¡¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _clean_title(title: str) -> str:
    title = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›\u201c\u201d\u2018\u2019ã€ã€‘ã€Šã€‹ï¼ˆï¼‰\s|ï½œÂ·â€¦]', ' ', title)
    title = re.sub(r'[,.\\!?:;\'"()\[\]{}\\-]', ' ', title)
    for prefix in ["æœ€æ–°", "é‡ç£…", "çªå‘", "åˆšåˆš", "ç‹¬å®¶", "æ·±åº¦"]:
        title = title.replace(prefix, "")
    return title.strip()


def _title_similarity(a: str, b: str) -> float:
    ca, cb = _clean_title(a), _clean_title(b)
    ratio = SequenceMatcher(None, ca, cb).ratio()

    words_a = set(re.findall(r'[\u4e00-\u9fff]{2,}|[A-Za-z]{2,}', ca))
    words_b = set(re.findall(r'[\u4e00-\u9fff]{2,}|[A-Za-z]{2,}', cb))
    if words_a and words_b:
        overlap = len(words_a & words_b)
        union = len(words_a | words_b)
        jaccard = overlap / union if union > 0 else 0
        ratio = max(ratio, jaccard)

    return ratio


def merge_similar_articles(articles: list[dict]) -> list[list[dict]]:
    n = len(articles)
    visited = [False] * n
    groups = []

    for i in range(n):
        if visited[i]:
            continue
        group = [articles[i]]
        visited[i] = True
        for j in range(i + 1, n):
            if visited[j]:
                continue
            sim = _title_similarity(articles[i]["title"], articles[j]["title"])
            if sim >= SIMILARITY_THRESHOLD:
                group.append(articles[j])
                visited[j] = True
        group.sort(key=lambda a: len(a.get("digest", "")), reverse=True)
        groups.append(group)

    return groups


# â”€â”€ è¯„åˆ†ä¸ç”Ÿæˆç®€æŠ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# æ¥æºæ·±åº¦åˆ†çº§
TIER1_SOURCES = {"æ™šç‚¹LatePost", "æ™šç‚¹AI", "ç”²å­å…‰å¹´", "è™å—…APP", "çˆ±èŒƒå„¿"}
TIER2_SOURCES = {"æœºå™¨ä¹‹å¿ƒ", "æå®¢å…¬å›­", "InfoQ", "Founder Park"}

CLICKBAIT_WORDS = ["éœ‡æƒŠ", "åˆšåˆš", "é‡ç£…", "çªå‘", "ç–¯äº†", "ç‚¸äº†", "æ²¸è…¾"]


def _load_reading_profile() -> dict | None:
    """åŠ è½½ç”¨æˆ·é˜…è¯»ç”»åƒï¼ˆå¦‚å­˜åœ¨ï¼‰"""
    if READING_PROFILE_FILE.exists():
        try:
            with open(READING_PROFILE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return None


def score_article(article: dict, group_size: int) -> float:
    """ä¸ºæ–‡ç« /ç»„æ‰“åˆ†ï¼Œç”¨äºç®€æŠ¥æ’åº"""
    score = 0.0
    title = article.get("title", "")
    digest = article.get("digest", "")
    nickname = article.get("nickname", "")

    # å†…å®¹ä¸°å¯Œåº¦ï¼šdigest é•¿åº¦
    if len(digest) > 80:
        score += 2
    elif len(digest) > 40:
        score += 1

    # æ¥æºæ·±åº¦åˆ†çº§
    if nickname in TIER1_SOURCES:
        score += 3
    elif nickname in TIER2_SOURCES:
        score += 2
    else:
        score += 1

    # ç‹¬å®¶æ€§ï¼šåªæœ‰ 1 å®¶æŠ¥é“ > å¤šå®¶åŒé¢˜
    if group_size == 1:
        score += 2

    # å¤šå®¶æŠ¥é“ä¹Ÿæœ‰ä»·å€¼ï¼ˆçƒ­åº¦åŠ åˆ†ï¼Œä½†ä¸å¦‚ç‹¬å®¶ï¼‰
    if group_size >= 3:
        score += 1

    # æ ‡é¢˜å…šæƒ©ç½š
    if any(w in title for w in CLICKBAIT_WORDS):
        score -= 1

    # ç”¨æˆ·å…´è¶£åŠ æƒï¼ˆreading_profile.jsonï¼‰
    profile = _load_reading_profile()
    if profile:
        sp = profile.get("source_preference", {})
        if nickname in sp:
            pref = sp[nickname]
            read_rate = pref.get("read", 0) / max(pref.get("offered", 1), 1)
            if read_rate > 0.3:
                score += 2
            elif read_rate > 0.15:
                score += 1

        topic_kw = profile.get("topic_keywords", {})
        for kw in topic_kw:
            if kw in title:
                score += 1
                break  # æœ€å¤šåŠ  1 åˆ†

    return score


def generate_digest(
    groups: list[list[dict]],
    hours: int,
    total_raw: int,
    total_ai: int,
) -> str:
    today = datetime.now().strftime("%Y-%m-%d")

    # æŒ‰ score æ’åº
    scored = []
    for group in groups:
        best = group[0]
        s = score_article(best, len(group))
        scored.append((s, group))
    scored.sort(key=lambda x: x[0], reverse=True)

    TOP_N = 5
    top_groups = scored[:TOP_N]
    rest_groups = scored[TOP_N:]

    # çƒ­ç‚¹è¯é¢˜ï¼šgroup_size >= 3
    hot_topics = []
    for s, group in scored:
        if len(group) >= 3:
            best = group[0]
            sources = [a.get("nickname", "") for a in group]
            hot_topics.append({"title": best["title"], "count": len(group), "sources": sources})

    lines = []
    lines.append(f"# AI æ—¥æŠ¥ {today}")
    lines.append("")
    lines.append(
        f"> {total_raw} ç¯‡ â†’ AI {total_ai} ç¯‡ â†’ åˆå¹¶ {len(groups)} æ¡ ï½œ æœ€è¿‘ {hours}h ï½œ {datetime.now().strftime('%H:%M')}"
    )
    lines.append("")

    # â”€â”€ å¿…è¯»åŒº â”€â”€
    lines.append(f"## å¿…è¯» (Top {len(top_groups)})")
    lines.append("")

    global_idx = 0
    for _score, group in top_groups:
        global_idx += 1
        best = group[0]
        nickname = best.get("nickname", "")
        dt = datetime.fromtimestamp(best["create_time"]).strftime("%m-%d")

        summary_parts = [f"<b>{global_idx}.</b> {best['title']}"]
        if nickname:
            summary_parts.append(f" <code>{nickname}</code>")
        summary_parts.append(f" <code>{dt}</code>")
        if len(group) > 1:
            summary_parts.append(f" <code>+{len(group) - 1}åŒé¢˜</code>")

        lines.append(f'<details><summary>{"".join(summary_parts)}</summary>')
        lines.append("")

        if best.get("digest"):
            lines.append(f"> {best['digest']}")
            lines.append("")

        lines.append(f"[åŸæ–‡]({best['link']})")

        if len(group) > 1:
            lines.append("")
            lines.append("åŒé¢˜æŠ¥é“ï¼š")
            for alt in group[1:]:
                alt_name = alt.get("nickname", "åŒé¢˜")
                lines.append(f"- [{alt_name}]({alt['link']})")

        lines.append("")
        lines.append("</details>")
        lines.append("")

    # â”€â”€ é€Ÿè§ˆåŒº â”€â”€
    if rest_groups:
        lines.append("## é€Ÿè§ˆ")
        lines.append("")
        lines.append("| # | æ ‡é¢˜ | æ¥æº | æ—¥æœŸ |")
        lines.append("|---|------|------|------|")

        for _score, group in rest_groups:
            global_idx += 1
            best = group[0]
            nickname = best.get("nickname", "")
            dt = datetime.fromtimestamp(best["create_time"]).strftime("%m-%d")
            title_link = f"[{best['title']}]({best['link']})"
            extra = f" +{len(group)-1}åŒé¢˜" if len(group) > 1 else ""
            lines.append(f"| {global_idx} | {title_link}{extra} | {nickname} | {dt} |")

        lines.append("")

    # â”€â”€ çƒ­ç‚¹è¯é¢˜åŒº â”€â”€
    if hot_topics:
        lines.append("## çƒ­ç‚¹è¯é¢˜")
        lines.append("")
        for topic in hot_topics:
            lines.append(f"**{topic['title'][:30]}{'...' if len(topic['title']) > 30 else ''}** ({topic['count']} ç¯‡æŠ¥é“)")
            lines.append(f"- {', '.join(topic['sources'])}")
            lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("*å…¥åº“ï¼šæŠŠé“¾æ¥å‘ç»™ Claude â†’ `å¸®å¿™åŠ ä¸‹ <url>`*")

    return "\n".join(lines)


# â”€â”€ é˜…è¯»åé¦ˆé—­ç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_digest_articles(digest_path: Path) -> dict[int, dict]:
    """ä» digest æ–‡ä»¶è§£æç¼–å·â†’æ–‡ç« æ˜ å°„ï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼"""
    articles = {}
    content = digest_path.read_text(encoding="utf-8")

    # æ–°æ ¼å¼ï¼šå¿…è¯»åŒº <details> + é€Ÿè§ˆåŒºè¡¨æ ¼
    # åŒ¹é… <b>N.</b> æ ‡é¢˜ <code>æ¥æº</code>
    detail_pattern = re.compile(
        r'<b>(\d+)\.</b>\s*(.+?)\s*<code>([^<]+)</code>\s*<code>(\d{2}-\d{2})</code>'
    )
    link_pattern = re.compile(r'\[åŸæ–‡\]\(([^)]+)\)')

    # å…ˆå¤„ç† <details> å—
    blocks = content.split("<details>")
    for block in blocks[1:]:  # skip first part before any <details>
        header_m = detail_pattern.search(block)
        link_m = link_pattern.search(block)
        if header_m:
            num = int(header_m.group(1))
            title = header_m.group(2).strip()
            source = header_m.group(3).strip()
            link = link_m.group(1) if link_m else ""
            articles[num] = {"title": title, "source": source, "link": link}

    # é€Ÿè§ˆåŒºè¡¨æ ¼è¡Œ: | N | [æ ‡é¢˜](link) | æ¥æº | æ—¥æœŸ |
    table_pattern = re.compile(
        r'^\|\s*(\d+)\s*\|\s*\[([^\]]+)\]\(([^)]+)\)[^|]*\|\s*([^|]+)\|\s*(\d{2}-\d{2})\s*\|',
        re.MULTILINE,
    )
    for m in table_pattern.finditer(content):
        num = int(m.group(1))
        if num not in articles:
            articles[num] = {
                "title": m.group(2).strip(),
                "source": m.group(4).strip(),
                "link": m.group(3).strip(),
            }

    return articles


def _update_reading_profile(read_articles: list[dict]):
    """æ›´æ–° reading_profile.json"""
    profile = {}
    if READING_PROFILE_FILE.exists():
        try:
            with open(READING_PROFILE_FILE, "r", encoding="utf-8") as f:
                profile = json.load(f)
        except Exception:
            pass

    profile.setdefault("total_offered", 0)
    profile.setdefault("total_read", 0)
    profile.setdefault("source_preference", {})
    profile.setdefault("topic_keywords", {})

    for art in read_articles:
        profile["total_read"] += 1
        source = art.get("source", "")
        if source:
            sp = profile["source_preference"].setdefault(source, {"offered": 0, "read": 0})
            sp["read"] += 1

        # æå–æ ‡é¢˜å…³é”®è¯ï¼ˆä¸­æ–‡ 2-4 å­—ï¼Œè‹±æ–‡ 2+ å­—æ¯ï¼‰
        title = art.get("title", "")
        words = _extract_keywords(title)
        for w in words:
            profile["topic_keywords"][w] = profile["topic_keywords"].get(w, 0) + 1

    profile["last_updated"] = datetime.now().strftime("%Y-%m-%d")

    with open(READING_PROFILE_FILE, "w", encoding="utf-8") as f:
        json.dump(profile, f, ensure_ascii=False, indent=2)


def _update_offered_count(num_articles: int, sources: list[str]):
    """åœ¨æ¯æ¬¡ç”Ÿæˆ digest åæ›´æ–° offered è®¡æ•°"""
    profile = {}
    if READING_PROFILE_FILE.exists():
        try:
            with open(READING_PROFILE_FILE, "r", encoding="utf-8") as f:
                profile = json.load(f)
        except Exception:
            pass

    profile.setdefault("total_offered", 0)
    profile.setdefault("total_read", 0)
    profile.setdefault("source_preference", {})
    profile.setdefault("topic_keywords", {})

    profile["total_offered"] += num_articles
    for src in sources:
        sp = profile["source_preference"].setdefault(src, {"offered": 0, "read": 0})
        sp["offered"] += 1

    profile["last_updated"] = datetime.now().strftime("%Y-%m-%d")

    with open(READING_PROFILE_FILE, "w", encoding="utf-8") as f:
        json.dump(profile, f, ensure_ascii=False, indent=2)


def cmd_mark_read(date: str, numbers: list[int]):
    """æ ‡è®°æŸæ—¥ç®€æŠ¥ä¸­çš„æŒ‡å®šæ–‡ç« ä¸ºå·²è¯»"""
    digest_path = DIGEST_DIR / f"{date}.md"
    if not digest_path.exists():
        print(f"æ‰¾ä¸åˆ°ç®€æŠ¥: {digest_path}")
        return

    articles_map = _parse_digest_articles(digest_path)
    if not articles_map:
        print(f"æœªèƒ½ä» {digest_path} è§£æå‡ºæ–‡ç« ")
        return

    read_articles = []
    for n in numbers:
        if n in articles_map:
            read_articles.append({"number": n, **articles_map[n]})
        else:
            print(f"  ç¼–å· {n} æœªæ‰¾åˆ°ï¼Œè·³è¿‡")

    if not read_articles:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« ç¼–å·")
        return

    # å†™å…¥ reading_log.jsonl
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    log_entry = {
        "date": date,
        "read_at": datetime.now().isoformat(),
        "articles": read_articles,
    }
    with open(READING_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    # æ›´æ–° articles_index.json ä¸­çš„ read_status
    try:
        index = load_articles_index()
        read_titles = {a["title"] for a in read_articles}
        updated = 0
        for art in index["articles"]:
            if art["title"] in read_titles:
                art["read_status"] = "read"
                updated += 1
        if updated:
            with open(ARTICLES_INDEX_FILE, "w", encoding="utf-8") as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    # æ›´æ–° reading_profile.json
    _update_reading_profile(read_articles)

    print(f"å·²æ ‡è®° {len(read_articles)} ç¯‡æ–‡ç« ä¸ºå·²è¯»:")
    for a in read_articles:
        print(f"  #{a['number']} {a['title'][:40]} ({a.get('source', '')})")


def cmd_stats():
    """è¾“å‡ºé˜…è¯»ç»Ÿè®¡"""
    if not READING_LOG_FILE.exists():
        print("æš‚æ— é˜…è¯»è®°å½•ã€‚ä½¿ç”¨ --mark-read <date> <numbers> æ ‡è®°å·²è¯»æ–‡ç« ã€‚")
        return

    # è¯»å–æ‰€æœ‰ reading log
    entries = []
    with open(READING_LOG_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    entries.append(json.loads(line))
                except Exception:
                    pass

    # æœ€è¿‘ 30 å¤©
    cutoff = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
    recent = [e for e in entries if e.get("date", "") >= cutoff]

    # ç»Ÿè®¡ digest ç¯‡æ•°
    digest_files = sorted(DIGEST_DIR.glob("????-??-??.md"))
    recent_digests = [f for f in digest_files if f.stem >= cutoff]
    total_offered = 0
    for df in recent_digests:
        articles = _parse_digest_articles(df)
        total_offered += len(articles)

    # å·²è¯»æ–‡ç« 
    all_read = []
    source_stats = {}
    topic_stats = {}
    for entry in recent:
        for art in entry.get("articles", []):
            all_read.append(art)
            src = art.get("source", "æœªçŸ¥")
            source_stats.setdefault(src, {"read": 0})
            source_stats[src]["read"] += 1

            # æå–å…³é”®è¯
            title = art.get("title", "")
            words = _extract_keywords(title)
            for w in words:
                topic_stats[w] = topic_stats.get(w, 0) + 1

    total_read = len(all_read)
    pct = f"{total_read / total_offered * 100:.1f}%" if total_offered > 0 else "N/A"

    print(f"é˜…è¯»ç»Ÿè®¡ (æœ€è¿‘ 30 å¤©):")
    print(f"  æ—¥æŠ¥ç¯‡æ•°: {total_offered} ç¯‡")
    print(f"  å·²è¯»: {total_read} ç¯‡ ({pct})")
    print()

    # æ¥æº Top 5 â€” è®¡ç®— offered per source from digests
    source_offered = {}
    for df in recent_digests:
        articles = _parse_digest_articles(df)
        for art in articles.values():
            src = art.get("source", "æœªçŸ¥")
            source_offered[src] = source_offered.get(src, 0) + 1

    print("  æœ€å¸¸è¯»æ¥æº Top 5:")
    sorted_sources = sorted(source_stats.items(), key=lambda x: x[1]["read"], reverse=True)[:5]
    for i, (src, data) in enumerate(sorted_sources, 1):
        offered = source_offered.get(src, "?")
        read_count = data["read"]
        rate = f"{read_count / offered * 100:.0f}%" if isinstance(offered, int) and offered > 0 else "?"
        print(f"    {i}. {src:<16} {read_count}/{offered} ({rate})")

    print()
    print("  æœ€å¸¸è¯»è¯é¢˜ Top 5:")
    sorted_topics = sorted(topic_stats.items(), key=lambda x: x[1], reverse=True)[:5]
    for i, (topic, count) in enumerate(sorted_topics, 1):
        print(f"    {i}. {topic:<16} {count} æ¬¡")


# â”€â”€ é€‰é¢˜è¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_keywords(title: str) -> set[str]:
    """ä»æ ‡é¢˜æå–å…³é”®è¯ï¼ˆä¸­æ–‡ 2-4 å­—è¯ã€è‹±æ–‡ tokenï¼‰"""
    words = set(re.findall(r'[\u4e00-\u9fff]{2,4}|[A-Za-z][A-Za-z0-9.]*(?:\s+[0-9.]+)?', title))
    stopwords = {"åˆšåˆš", "é‡ç£…", "æœ€æ–°", "çªå‘", "ç‹¬å®¶", "ä¸€ä¸ª", "ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•",
                 "å°±æ˜¯", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "è¿˜æ˜¯", "å·²ç»", "ç»ˆäº", "å±…ç„¶", "ç«Ÿç„¶",
                 "ä¸ºä»€ä¹ˆ", "å…³äº", "ä½†æ˜¯", "å› ä¸º", "æ‰€ä»¥", "ä¸æ˜¯", "åªæ˜¯", "è¿˜æœ‰", "ç„¶è€Œ",
                 "æ¥äº†", "å‡ºäº†", "çœ‹çœ‹", "æˆ‘ä»¬", "ä»–ä»¬", "è‡ªå·±", "çœŸçš„", "åˆ°åº•",
                 "the", "and", "for", "with", "from", "that", "this", "are", "was",
                 "not", "but", "all", "has", "had", "will", "how", "can", "its"}
    return {w.strip() for w in words if w.strip() not in stopwords and len(w.strip()) >= 2}


def cmd_trends():
    """è¾“å‡ºæœ€è¿‘ 3 å¤©çš„çƒ­ç‚¹è¶‹åŠ¿"""
    digest_files = sorted(DIGEST_DIR.glob("????-??-??.md"), reverse=True)[:3]
    if not digest_files:
        print("æ²¡æœ‰æ‰¾åˆ° digest æ–‡ä»¶")
        return

    today_file = digest_files[0] if digest_files else None
    today_stem = today_file.stem if today_file else ""
    older_stems = {f.stem for f in digest_files[1:]}

    all_keywords = {}  # keyword â†’ count
    today_keywords = set()
    older_keywords = set()
    total_articles = 0

    for df in digest_files:
        articles = _parse_digest_articles(df)
        total_articles += len(articles)
        for art in articles.values():
            words = _extract_keywords(art.get("title", ""))
            for w in words:
                all_keywords[w] = all_keywords.get(w, 0) + 1
                if df.stem == today_stem:
                    today_keywords.add(w)
                else:
                    older_keywords.add(w)

    # æ’åºï¼Œæ ‡æ³¨ NEW
    sorted_kw = sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)

    days = len(digest_files)
    print(f"çƒ­ç‚¹è¶‹åŠ¿ (æœ€è¿‘ {days} å¤©, {total_articles} ç¯‡):")
    print()

    max_count = sorted_kw[0][1] if sorted_kw else 1
    for kw, count in sorted_kw[:15]:
        bar_len = int(count / max_count * 12)
        bar = "â–ˆ" * bar_len

        is_new = kw in today_keywords and kw not in older_keywords and days > 1
        if is_new:
            label = "ğŸ†•"
        elif count >= 5:
            label = "ğŸ”¥"
        else:
            label = "  "

        new_tag = "  â† æ–°è¯é¢˜" if is_new else ""
        print(f"  {label} {kw:<16} {count} æ¬¡   {bar}{new_tag}")


def cmd_search(keyword: str):
    """æœç´¢çŸ¥è¯†åº“å’Œè¿‘æœŸç®€æŠ¥ä¸­åŒ¹é…çš„æ–‡ç« """
    kw_lower = keyword.lower()
    results_kb = []
    results_digest = []

    # 1. æœç´¢ articles_index.json
    try:
        index = load_articles_index()
        for art in index["articles"]:
            title = art.get("title", "")
            tags = " ".join(art.get("tags", []))
            if kw_lower in title.lower() or kw_lower in tags.lower():
                results_kb.append(art)
    except Exception:
        pass

    # 2. æœç´¢æœ€è¿‘ 7 å¤© digest
    digest_files = sorted(DIGEST_DIR.glob("????-??-??.md"), reverse=True)[:7]
    for df in digest_files:
        articles = _parse_digest_articles(df)
        for num, art in sorted(articles.items()):
            if kw_lower in art.get("title", "").lower():
                results_digest.append({"date": df.stem, "number": num, **art})

    total = len(results_kb) + len(results_digest)
    print(f'æœç´¢: "{keyword}" ({total} æ¡åŒ¹é…)')
    print()

    if results_kb:
        print("  çŸ¥è¯†åº“:")
        for art in results_kb:
            status = "å·²è¯»" if art.get("read_status") == "read" else "æœªè¯»"
            print(f"    [{status}] {art['title']} ({art.get('author', '')}, {art.get('crawl_date', '')})")
            print(f"           â†’ knowledge_base/{art.get('path', '')}/")
        print()

    if results_digest:
        print("  è¿‘æœŸç®€æŠ¥:")
        for art in results_digest:
            print(f"    {art['date']} #{art['number']}  {art['title'][:50]} ({art.get('source', '')})")


# â”€â”€ digest-site æ„å»º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_build_site():
    """ç”Ÿæˆ/æ›´æ–° digest-site é™æ€ç«™"""
    site_digests = SITE_DIR / "digests"
    site_digests.mkdir(parents=True, exist_ok=True)

    # 1. å¤åˆ¶æ‰€æœ‰ YYYY-MM-DD.md åˆ° site_digests/
    copied = 0
    for f in DIGEST_DIR.glob("????-??-??.md"):
        shutil.copy2(f, site_digests / f.name)
        copied += 1

    # 2. ç”Ÿæˆ index.json
    dates = sorted([f.stem for f in site_digests.glob("????-??-??.md")], reverse=True)
    index = [{"date": d} for d in dates]
    (SITE_DIR / "index.json").write_text(json.dumps(index, ensure_ascii=False, indent=2))

    # 3. ç¡®ä¿ index.html å­˜åœ¨ï¼ˆé¦–æ¬¡ä»åŒç›®å½•æ¨¡æ¿å¤åˆ¶ï¼‰
    html_path = SITE_DIR / "index.html"
    if not html_path.exists():
        print(f"è­¦å‘Š: {html_path} ä¸å­˜åœ¨ï¼Œè¯·æ‰‹åŠ¨æ”¾ç½® index.html")

    print(f"digest-site å·²æ›´æ–°: {copied} ç¯‡ â†’ {SITE_DIR}")

    # 4. éƒ¨ç½²åˆ° Vercel
    if shutil.which("vercel") and (SITE_DIR / ".vercel").is_dir():
        print("æ­£åœ¨éƒ¨ç½²åˆ° Vercel...")
        result = subprocess.run(
            ["vercel", "--yes", "--prod"],
            cwd=SITE_DIR,
            capture_output=True,
            text=True,
            timeout=120,
        )
        if result.returncode == 0:
            # ä»è¾“å‡ºä¸­æå– URL
            for line in result.stdout.splitlines():
                if "https://" in line and "vercel.app" in line:
                    print(f"å·²éƒ¨ç½²: {line.strip()}")
                    break
            else:
                print("Vercel éƒ¨ç½²å®Œæˆ")
        else:
            print(f"Vercel éƒ¨ç½²å¤±è´¥: {result.stderr.strip()}")
    elif not shutil.which("vercel"):
        print("è·³è¿‡éƒ¨ç½²: vercel CLI æœªå®‰è£…")
    else:
        print("è·³è¿‡éƒ¨ç½²: å°šæœªåˆå§‹åŒ– Vercel é¡¹ç›®ï¼ˆå…ˆæ‰‹åŠ¨è¿è¡Œ vercel ä¸€æ¬¡ï¼‰")


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def manage_accounts(args):
    """ç®¡ç†å…³æ³¨çš„å…¬ä¼—å·åˆ—è¡¨"""
    if args.list_accounts:
        accounts = get_followed_accounts()
        print(f"å…³æ³¨åˆ—è¡¨ ({len(accounts)}):")
        for a in accounts:
            print(f"  - {a}")
        return True

    if args.add_account:
        try:
            r = requests.post(FOLLOWED_ACCOUNTS_API, json={"add": args.add_account}, timeout=5)
            data = r.json()
            print(f"å·²æ·»åŠ : {args.add_account}")
            print(f"å½“å‰åˆ—è¡¨: {data.get('accounts', [])}")
        except Exception as e:
            print(f"æ·»åŠ å¤±è´¥: {e}")
        return True

    if args.remove_account:
        try:
            r = requests.post(FOLLOWED_ACCOUNTS_API, json={"remove": args.remove_account}, timeout=5)
            data = r.json()
            print(f"å·²ç§»é™¤: {args.remove_account}")
            print(f"å½“å‰åˆ—è¡¨: {data.get('accounts', [])}")
        except Exception as e:
            print(f"ç§»é™¤å¤±è´¥: {e}")
        return True

    if args.sync_accounts:
        sync_accounts_from_chrome()
        return True

    return False


def main():
    parser = argparse.ArgumentParser(description="AI æ—¥æŠ¥ç®€æŠ¥ç”Ÿæˆå™¨")
    parser.add_argument("--hours", type=int, default=24, help="æ‰«ææœ€è¿‘ N å°æ—¶ï¼ˆé»˜è®¤ 24ï¼‰")
    parser.add_argument("--list-accounts", action="store_true", help="åˆ—å‡ºå…³æ³¨çš„å…¬ä¼—å·")
    parser.add_argument("--add-account", type=str, help="æ·»åŠ å…¬ä¼—å·")
    parser.add_argument("--remove-account", type=str, help="ç§»é™¤å…¬ä¼—å·")
    parser.add_argument("--sync-accounts", action="store_true", help="ä» Chrome exporter åŒæ­¥å…¬ä¼—å·åˆ—è¡¨")
    # Feature 3: é˜…è¯»åé¦ˆ
    parser.add_argument("--mark-read", nargs="+", metavar=("DATE", "NUM"), help="æ ‡è®°å·²è¯»: --mark-read 2026-02-20 1 3 5")
    parser.add_argument("--stats", action="store_true", help="è¾“å‡ºé˜…è¯»ç»Ÿè®¡")
    # Feature 4: é€‰é¢˜è¾…åŠ©
    parser.add_argument("--trends", action="store_true", help="è¾“å‡ºæœ€è¿‘ 3 å¤©çƒ­ç‚¹è¶‹åŠ¿")
    parser.add_argument("--search", type=str, metavar="KEYWORD", help="æœç´¢çŸ¥è¯†åº“å’Œç®€æŠ¥ä¸­çš„æ–‡ç« ")
    parser.add_argument("--build-site", action="store_true", help="æ„å»º/æ›´æ–° digest-site é™æ€ç«™")
    args = parser.parse_args()

    # Feature 3 å­å‘½ä»¤
    if args.mark_read:
        date = args.mark_read[0]
        numbers = [int(n) for n in args.mark_read[1:]]
        cmd_mark_read(date, numbers)
        return

    if args.stats:
        cmd_stats()
        return

    # Feature 4 å­å‘½ä»¤
    if args.trends:
        cmd_trends()
        return

    if args.search:
        cmd_search(args.search)
        return

    if args.build_site:
        cmd_build_site()
        return

    if manage_accounts(args):
        return

    log("=" * 60)
    log("AI æ—¥æŠ¥ç®€æŠ¥ å¼€å§‹ç”Ÿæˆ")
    log("=" * 60)

    # 1. æ£€æŸ¥ exporter
    if not check_exporter():
        log("exporter æœåŠ¡ä¸å¯è¾¾ (localhost:3000)ï¼Œé€€å‡º")
        notify("AI æ—¥æŠ¥å¤±è´¥", "exporter ä¸å¯è¾¾ (localhost:3000)")
        sys.exit(1)
    log("exporter æœåŠ¡æ­£å¸¸")

    # 2. è·å– auth-key
    try:
        auth_key = get_auth_key()
        log(f"auth-key å·²è·å– ({auth_key[:8]}...)")
    except Exception as e:
        log(f"è·å– auth-key å¤±è´¥: {e}")
        log("è¯·å…ˆåœ¨æµè§ˆå™¨ä¸­ç™»å½• exporter (localhost:3000)")
        notify("AI æ—¥æŠ¥å¤±è´¥", "auth-key è·å–å¤±è´¥ï¼Œè¯·ç™»å½• exporter")
        sys.exit(1)

    # 2.5 å¥åº·æ£€æŸ¥ï¼šéªŒè¯ auth-key å®é™…å¯ç”¨
    try:
        test_r = _api_get(
            f"{EXPORTER_BASE}/api/web/mp/searchbiz",
            params={"keyword": "è™å—…"},
            headers={"X-Auth-Key": auth_key},
        )
        test_data = test_r.json()
        if not test_data.get("list"):
            raise ValueError("auth-key éªŒè¯å¤±è´¥ï¼šæœç´¢è¿”å›ç©ºç»“æœ")
        log("auth-key éªŒè¯é€šè¿‡")
    except Exception as e:
        log(f"auth-key éªŒè¯å¤±è´¥: {e}")
        notify("AI æ—¥æŠ¥å¤±è´¥", "auth-key è¿‡æœŸæˆ–æ— æ•ˆ")
        sys.exit(1)

    # 3. æ‹‰å–æ‰€æœ‰å…¬ä¼—å·æ–‡ç« 
    raw_articles = fetch_all_articles(auth_key, args.hours)
    if not raw_articles:
        log("æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘çš„æ–‡ç« ï¼Œé€€å‡º")
        return

    total_raw = len(raw_articles)
    log(f"åŸå§‹æ–‡ç« : {total_raw} ç¯‡ï¼ˆæ¥è‡ª {len(set(a['nickname'] for a in raw_articles))} ä¸ªå…¬ä¼—å·ï¼‰")

    # 4. AI ç›¸å…³æ€§è¿‡æ»¤
    ai_articles = [a for a in raw_articles if is_ai_related(a["title"], a.get("digest", ""))]
    total_ai = len(ai_articles)
    log(f"AI ç›¸å…³: {total_ai}/{total_raw}")

    if not ai_articles:
        log("æ²¡æœ‰ AI ç›¸å…³æ–‡ç« ï¼Œé€€å‡º")
        return

    ai_articles.sort(key=lambda a: a.get("create_time", 0), reverse=True)

    # 5. å»æ‰å·²å…¥åº“çš„
    index = load_articles_index()
    existing_titles = {a["title"] for a in index["articles"]}
    existing_urls = set()
    for a in index["articles"]:
        meta_path = KNOWLEDGE_BASE_DIR / a["path"] / "metadata.json"
        if meta_path.exists():
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                    if meta.get("source_url"):
                        existing_urls.add(meta["source_url"])
            except Exception:
                pass

    fresh = []
    for a in ai_articles:
        if a["link"] in existing_urls or a["title"] in existing_titles:
            continue
        fresh.append(a)

    log(f"å»å·²å…¥åº“: {len(fresh)} ç¯‡ï¼ˆå·²è·³è¿‡ {total_ai - len(fresh)} ç¯‡ï¼‰")

    if not fresh:
        log("å…¨éƒ¨å·²å…¥åº“ï¼Œæ— éœ€ç”Ÿæˆç®€æŠ¥")
        return

    # 6. åˆå¹¶åŒç±»é¡¹
    groups = merge_similar_articles(fresh)
    log(f"åˆå¹¶åŒç±»é¡¹: {len(fresh)} â†’ {len(groups)} æ¡")

    # 7. ç”Ÿæˆç®€æŠ¥
    digest_md = generate_digest(groups, args.hours, total_raw, total_ai)

    DIGEST_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    digest_path = DIGEST_DIR / f"{today}.md"
    with open(digest_path, "w", encoding="utf-8") as f:
        f.write(digest_md)

    log(f"ç®€æŠ¥å·²ç”Ÿæˆ: {digest_path}")
    notify("AI æ—¥æŠ¥", f"{len(groups)} æ¡æ–°æ–‡ç«  â†’ digests/{today}.md")

    # æ›´æ–° reading_profile çš„ offered è®¡æ•°
    sources = [g[0].get("nickname", "") for g in groups if g]
    _update_offered_count(len(groups), sources)

    # è‡ªåŠ¨æ›´æ–° digest-site
    cmd_build_site()

    devlog({
        "type": "task",
        "context": "ai_digest",
        "action": f"ç”Ÿæˆ AI æ—¥æŠ¥ ({args.hours}h)",
        "result": f"å…¨éƒ¨ {total_raw} â†’ AI {total_ai} â†’ å»é‡ {len(fresh)} â†’ åˆå¹¶ {len(groups)} æ¡",
    })


if __name__ == "__main__":
    main()
