# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

IndividualRL - A Python-based project with two core focuses:

### ğŸ¯ PRIMARY TASK: Personal Knowledge Base System
**This is the CORE mission** - Building an automated personal knowledge management system:
- **Article Collection**: Crawl WeChat articles and other content
- **Auto Classification**: 7 categories (äººå·¥æ™ºèƒ½, å•†ä¸š, é‡‘è, ä¸ªäººæˆé•¿, å†å², å“²å­¦, æ–‡å­¦)
- **Interactive Q&A**: Answer user questions based on article content, record conversations
- **Notion Integration**: Future sync to user's Notion database

ğŸ“‹ **See KNOWLEDGE_BASE_PLAN.md for complete roadmap and implementation details**

### Secondary: Reinforcement Learning
Future RL experiments and implementations

## å¾®ä¿¡æ–‡ç« æŠ“å–

å½“ç”¨æˆ·å‘æ¥å¾®ä¿¡æ–‡ç« é“¾æ¥ï¼ˆ`mp.weixin.qq.com`ï¼‰æ—¶ï¼Œä½¿ç”¨ `fetch_article.py` æŠ“å–ï¼š

```bash
source ~/venv/automation/bin/activate

# æŠ“å–æ–‡ç« ä¿å­˜ä¸º Markdownï¼ˆæŒ‡å®šè¾“å‡ºç›®å½•ï¼‰
python knowledgebase/wx-article-cron/fetch_article.py <url> -o <è¾“å‡ºç›®å½•>

# æŠ“å–å¤šç¯‡
python knowledgebase/wx-article-cron/fetch_article.py <url1> <url2> <url3> -o <è¾“å‡ºç›®å½•>

# å…¥åº“åˆ°çŸ¥è¯†åº“ï¼ˆè‡ªåŠ¨åˆ†ç±»åˆ° knowledge_base/{åˆ†ç±»}/{å…¬ä¼—å·}/{æ ‡é¢˜}/ï¼‰
python knowledgebase/wx-article-cron/fetch_article.py <url> --kb
```

**å‰ç½®æ¡ä»¶**: exporter æœåŠ¡éœ€è¿è¡Œåœ¨ localhost:3000ï¼ˆé€šå¸¸å·²åœ¨åå°è·‘ï¼‰ã€‚

**å¸¸è§åœºæ™¯**:
- ç”¨æˆ·è¯´ã€Œå¸®å¿™åŠ ä¸‹ <url>ã€â†’ ç”¨ `--kb` å…¥åº“
- ç”¨æˆ·è¯´ã€ŒæŠ“ä¸‹è¿™ç¯‡ã€â†’ å­˜åˆ°æŒ‡å®šç›®å½•æˆ–å½“å‰ç›®å½•
- ç”¨æˆ·ç»™å¤šä¸ªé“¾æ¥ â†’ æ‰¹é‡æŠ“å–

## User Preferences

**Model Selection Reminder**:
- Current model: Claude Sonnet 4.5 (suitable for most development tasks)
- **IMPORTANT**: If you encounter tasks that are beyond your capability or require exceptional reasoning/complexity, proactively remind the user: "This task might benefit from using Opus for better results"
- Examples: Complex architecture design, innovative solutions to novel problems, large-scale refactoring, advanced algorithm optimization

## Development Environment

### Python Environment
- Virtual environment located in `.venv/`
- Python version: 3.9 (based on .venv structure)

### Activating the Environment
```bash
source .venv/bin/activate  # On macOS/Linux
# or
.venv\Scripts\activate     # On Windows
```

## Project Structure

### Web Crawler (Current)
- `connect_chrome.py` - Main crawler using Chrome CDP
- `start_chrome_debug.sh` - Chrome debug mode launcher
- `get_chrome_cookies.py` - Cookie extraction utility
- `fetch_with_cookies.py` - Cookie-based crawler (backup)
- `WEBCRAWLER_NOTES.md` - Comprehensive crawler documentation

### RL Components (Future)
- Training scripts
- Environment definitions
- Agent/model implementations
- Configuration files

## Common Commands

### Web Crawler
```bash
# 1. Start Chrome in debug mode
./start_chrome_debug.sh

# 2. Open article in Chrome, then run crawler
source .venv/bin/activate
python connect_chrome.py
```

### Dependencies
```bash
pip install -r requirements.txt
playwright install chromium
```

## Important Notes

### Knowledge Base System
- **Read KNOWLEDGE_BASE_PLAN.md first** - Contains complete system design and roadmap
- Current phase: Building local knowledge base (Phase 1)
- Future phase: Notion integration (Phase 2)
- User expects: Article crawling â†’ Auto classification â†’ Q&A recording â†’ Notion sync

### Web Crawler
- See `WEBCRAWLER_NOTES.md` for detailed web crawling workflow and troubleshooting
- Chrome must run on port 9222 for CDP connection
- Crawler automatically detects verification pages (60s timeout)
