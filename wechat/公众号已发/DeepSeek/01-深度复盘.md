# 沉默的DeepSeek，到底在沉默什么？

2025年1月20日，DeepSeek R1上线。七天后的1月27日，美股科技股单日蒸发近万亿美元，仅英伟达一家就跌了5890亿——创下美股单只股票最大单日跌幅纪录。那一周，硅谷的科技记者们第一次认真拼写"DeepSeek"这个词。

![2025年1月27日，NVDA单日暴跌17%，市值蒸发5890亿美元（数据：Yahoo Finance）](images/nvidia_crash_jan27.png)

一年后的今天，全中国都在猜：V4什么时候出？

DeepSeek的回应：沉默。

**先看一组数据。**
R2跳票了。V4"据说"要来了。The Information报道说2月中旬发布，主攻Coding，内部测试超越Claude和GPT。但官方一个字没说。

与此同时，整个中国AI行业正在经历有史以来最拥挤的一个春节。

**春节前后，中国AI迎来史上最密集的发布潮。**

1月底，Kimi率先发布K2.5——万亿参数全能模型。进入2月，阿里千问App砸出30亿"春节免单"补贴，9小时订单破千万。2月7日，字节Seedance 2.0内测上线，有人喊出"文艺复兴和科技革命同时发生"。2月10日，阿里Qwen-Image 2.0发布。

然后是真正的高潮。2月11日深夜智谱GLM-5上线，744B参数，开源，四天内股价接近翻倍。第二天MiniMax发了M2.5，对标Claude Opus 4.6，市值冲破1800亿港元，摩根大通首覆直接给了"超配"。2月14日字节豆包2.0正式官宣。

**不到三周，五家公司，六个重磅产品。**

![智谱GLM-5上线，744B参数开源（来源：36氪）](images/glm5_release.png)

而DeepSeek在这一周做了什么？悄悄把上下文窗口从128K拉到了1M。没有发布会，没有公众号推送，没有热搜。

有人猜这是V4的Preview版本。官方照例沉默。

坦率说，如果只看热闹，DeepSeek好像已经不在牌桌上了。

**但仔细看牌面，会发现一件有意思的事。**

智谱GLM-5的核心注意力机制，采用了DeepSeek-V3.2引入的稀疏注意力技术DSA（DeepSeek Sparse Attention）——这是DeepSeek早期NSA研究的工程化演进版本。Kimi K2的基座架构，沿用了DeepSeek-V3的MoE混合专家和MLA多头潜注意力路线，在此基础上做了专家数量和注意力头的针对性调整。

这不是谁抄了谁的问题。开源技术被采用，本来就是开源的意义——就像所有人都用Transformer不代表所有人在抄Google，而是Google定义了一个时代的底层范式。

**DeepSeek做的是同样的事：用开源定义了这一代中国大模型的底层语法。**

当行业里最重要的几个玩家都在你写的规则上构建产品，你不需要自己开发布会。

不过这里有一个需要拆开说的地方。

智谱和月之暗面（Kimi）跟DeepSeek的关系，其实是两种完全不同的竞争。

**智谱更像是技术路线的继承者。** GLM-5在注意力机制上直接采用了DeepSeek的DSA，同时在参数规模工程、Multi-Token Prediction等方面做了自己的工作。744B总参数、78层网络、28.5T训练数据——这些不是开源能给的，是智谱自己的活。

**杨植麟则是另一种选手——他是在同一个擂台上跟梁文锋真正交手的人。**

回看过去一年，梁文锋和杨植麟撞过两次车。

第一次是2025年1月20日。DeepSeek R1和Kimi K1.5同日发布，两篇论文都指向了基于结果奖励模型的强化学习训练。R1从Zero开始训练、方法更纯粹、直接开源；K1.5在论文层面对"如何训练推理模型"的阐述反而更详尽，只是被R1的热度盖过了。在后续的第三方评测中，R1和K1.5常被与OpenAI o系列并列比较。

第二次是2025年2月18日——恰好一年前的今天。梁文锋发了NSA（原生稀疏注意力），杨植麟同日发了MoBA（混合块注意力）。两篇论文都指向learned sparse attention这个方向，但路径不同：NSA结构化更强，MoBA更灵活、更少先验假设，而且已经在Kimi产品中跑了一年。

清华的章明星在一篇分析中写过一句话，我觉得很精准：**"大模型这套架构似乎自己指出了前进的路线，让不同的人从不同的角度得出了相似的方向。"**

所以问题不是"杨植麟在用DeepSeek的东西"。在产品底座层面，Kimi K2确实站在DeepSeek-V3的肩膀上，这是开源生态的正常运作。但在研究前沿，杨植麟和梁文锋是真正的对手——各有胜负，各有原创。

**好，竞争格局看完了。回到那个核心问题：DeepSeek为什么沉默？**

至少有三股力量在起作用。

**第一，芯片卡脖子。** 英国金融时报2025年8月报道，DeepSeek原计划2025年5月发布的R2，因为华为昇腾芯片训练问题被迫推迟。华为派了工程师驻场，但进展缓慢——芯片稳定性、片间通信速度、软硬件适配都不到位。DeepSeek最终被迫继续用Nvidia做训练，华为仅用于推理。

这不是DeepSeek一家的问题，而是整个国产AI的系统性瓶颈。但DeepSeek比其他公司更在意这件事——因为他们不融资，不靠补贴，每一块芯片都是自己花的钱。用不好的卡训出不好的模型，不如不出。

**第二，梁文锋自己不满意。** 有报道提到DeepSeek内部对R2的进展不够满意，梁文锋不愿意出半成品。这跟他一贯的风格一致——R1上线前也经历了反复打磨，最终一出手就是开源+碾压级性能，没有"先发个Preview试试水"这种操作。

**第三，也是最重要的——战略转向。** 从V3到R1，DeepSeek一直是"基础模型V系列"和"推理模型R系列"双轨并行。但V3.1已经正式引入了混合推理架构——在同一个模型里同时支持快速响应和深度推理。V4大概率会把两条线彻底合并成统一架构。

这不是简单地"出下一代"，而是重新定义什么是一代。

**而沉默期的DeepSeek，远不是什么都没做。**

2025年末到2026年初，梁文锋署名的论文密集发出：

**Engram（条件记忆）**，把模型中的基础知识从"每次计算"变成"直接查表"——就像背下乘法表而不是每次都手算。省下的计算深度留给真正需要推理的难题。

**mHC（流形约束超连接）**，解决的是模型越大训练越不稳定的老大难问题，让性能可以随参数量线性扩展，而不是到了某个规模就塌掉。

再加上2025年初的NSA（原生稀疏注意力）——不让每个词跟所有词比较，用分层压缩+关键词选择+滑动窗口组合，只看最相关的部分，算力大幅降低，效果基本不损失。

**三篇论文合在一起，指向的不是一个新模型，而是一套新的训练范式。**

![DeepSeek NSA论文（arXiv, 2025年2月）](images/nsa_paper.png)

![DeepSeek Engram论文（arXiv, 2026年1月）](images/engram_paper.png)

与此同时，DeepSeek另一支团队连发两篇DeepSeek-OCR论文，做的事情更反直觉：把文本渲染成图像，再用视觉编码器压缩——压缩10倍，精度仍达97%。MIT Technology Review专门报道了这项技术。这类跨模态压缩能力，很可能是V4多模态输入的前置准备。

别人在优化产品，DeepSeek在重写规则。

前面说的"不出半成品"，不是策略，是性格。**理解梁文锋的沉默，得先理解梁文锋这个人。**

1985年，广东湛江吴川。吴川一中高考第一名，浙大信息与电子工程学院本硕。研究生期间带着8万元本金开始探索量化交易。2015年创立幻方量化，2021年管理规模突破千亿，成为国内量化私募"四大天王"之一。然后拿着幻方的利润，2023年成立DeepSeek。

**没有融资。没有投资人。没有外部董事会。**

他用"一只平凡的小猪"的名义匿名捐了1.38亿，知道这件事的人寥寥无几。2025年入选Nature年度十大科学人物，R1论文登上Nature封面。他依然没有接受过一次正式的公开采访。

![DeepSeek R1论文登上Nature封面](images/nature_cover.png)

![梁文锋入选Nature 2025年度十大科学人物，标签："Tech disruptor"](images/nature_10.png)

> "我们大部分公司习惯follow，而不是创新。"
>
> "开源不会失去什么。被follow，对技术人来说是一种成就。"

量化交易员的底层直觉是什么？**没有edge不下注。** 信号不够强，仓位就不动。别人疯狂交易的时候，量化交易员可能在等。等到信号出现，一击致命。

R1就是那一击。上一次"信号出现"是2025年1月。

下一次，也许快了。

**最后聊一个有争议的判断：DeepSeek的沉默，对中国AI到底是好事还是坏事？**

先说好的一面。过去一年，中国AI六小龙各自打出了自己的仗。智谱完成港股IPO，市值从518亿港元涨到1792亿。MiniMax市值破了1800亿，拿到摩根大通"超配"。月之暗面C轮融了5亿美元，现金储备超百亿。阿里Qwen系列在HuggingFace开源下载量长期排名全球第一，持续迭代，撑起了国内开源生态的另一根支柱。

这些成绩是各家自己挣来的，不是谁"让"出来的。

但另一面也成立。生态需要技术锚点。DeepSeek-V3和R1在架构层面的影响力有目共睹，而这套架构已经一年没更新了。Qwen在模型层持续迭代，但在底层架构创新上，行业仍然在等一个新的范式突破。

**真正让人期待的是第三种可能：如果V4出来，不仅开源，而且再次碾压闭源模型——那它动摇的就不只是中国AI的格局，而是OpenAI的商业模式本身。**

上次DeepSeek证明了"几十万美元能训出顶级推理模型"。如果这次再证明"开源模型在综合能力上也能比肩甚至超越闭源"，那"为什么要为GPT付费"就会从技术圈的小众讨论变成一个商业命题。

2025年1月，DeepSeek用一篇论文和一个开源模型震动了硅谷。

2026年2月，它用沉默震动了整个中国AI行业。

所有人都在等梁文锋说话。但也许这就是答案——**真正的竞争力不是发布频率，而是沉默时在想什么。**

当你写的规则已经成为行业的底层语法，你不需要每天开口。你只需要在对的时间，说对的那一句。

上一次他开口，英伟达单日蒸发了5890亿美元。

下一次，谁知道呢。

**下一次梁文锋开口，你猜会是什么？**

- V4开源，再次碾压闭源模型
- 一个全新架构，不叫V也不叫R
- 什么都不发，继续沉默到下半年
- 不是模型，是一个让所有人没想到的东西

*降临派手记 · 章北海执笔 · 2026年2月*

*数据来源：The Information、英国金融时报、Reuters、MIT Technology Review、虎嗅、36氪、新浪科技、快科技、澎湃新闻、Nature*
